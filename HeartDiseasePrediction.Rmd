---
title: "HeartDieasePrediction"
author: "Yunsheng Li & Yung-Lin Chang"
date: "November 21, 2017"
output:
  html_document: default
  pdf_document: default
---
### Introduction
Our project is mainly focused on the heart disease prediction by using the machine learning algorithm to explore whether the patient have heart disease or not. In the process of finding the meaningful model, we have done the data exploration, data preparation, model comparison and feature selection. After evaluating the effectiveness of all the options, we concluded the random forest model with all 14 features has the best performance in this prediction.

### Data Source and Method
The data was retrieved from UCI Machine Learning Repository http://archive.ics.uci.edu/ml/datasets/heart+Disease. Based on the dataset, the supervised learning algorithm and binary classification method was used to explore and generate the prediction model by R and R packages.

### The dataset
The dataset has 303 observations and 14 attributes which the 14th attribute "status" is the predictable attribute showing either absence or presence of the heart disease on the patient.
```{r}
hd <- read.table("heart.disease.txt", header = FALSE, sep = ",")
colnames(hd) <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "status")
## sex: (1 = male; 0 = female)
## cp: chest pain type: 
##       Value 1: typical angina
##       Value 2: atypical angina
##       Value 3: non-anginal pain
##       Value 4: asymptomatic
## trsetbps: resting blood pressure (mmHg)
## chol: serum cholesterol in mg/dl
## fbs: fasting blood sugar > 120 mg/dl (1 = true; 0 = false)
## restecg: resting electrocardiographic results: 
##       Value 0: normal
##       Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)
##       Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria
## thalach: maximum heart rate achieved
## exang: exercise induced angina (1 = yes; 0 = no)
## oldpeak: ST depression induced by exercise relative to rest
## slope: the slope of the peak exercise ST segment
##       Value 1: upsloping
##       Value 2: flat
##       Value 3: downsloping
## ca: number of major vessels (0-3) colored by fluoroscopy
## thal: 3 = normal; 6 = fixed defect; 7 = reversible defect
## status: diagnosis of heart disease 
##       Value < 0: < 50% diameter narrowing (Absence)
##       Value > 0: > 50% diameter narrowing (Presence)
```

### Data Preparation
#### /Missing Data/
* Packages used: **mice** (missing data imputation), **VIM** (missing data visulization)  

We first turned missing values "?" into NA. 6 missing values were found in the dataset which located in attribute ca and thal as the shown graph generated by package VIM. We impute the missing data by mice package in the method of 'cart', which imputes univariate missing data using classification and regression trees.

```{r message=FALSE}
##turn missing value into NA
hd[hd == "?"] <- NA

## check the missing data pattern: only ca and thal have NA
library(mice)
md.pattern(hd)
library(VIM)
aggr(hd, col=c('navyblue','yellow'), numbers=TRUE,
      labels=names(hd), cex.axis=.7, gap=3, 
      ylab=c("Missing data","Pattern"))

## impute missing data 
imputed <- mice(hd, m=5, meth = 'cart',seed = 500)

##check the imputed data 
imputed$imp$ca
imputed$imp$thal

## complete dataset
new_hd <- complete(imputed, 1)
new_hd$ca <- as.numeric(as.character(new_hd$ca))
new_hd$thal <- as.numeric(as.character(new_hd$thal))
summary(new_hd)
nrow(new_hd[is.na(new_hd), ])
head(new_hd)
```
  
#### /Scale the feature/
* Packages used: **ggplot2** (powerful plotting package), **magrittr** (%>%, decrease development time and to improve readability and maintainability of code), **tidyr** (for gather() Function) 

Scale the data by the method of Z-Score normalization (data pre-processing). Density plots were representing how data distributed and the relationship between status and each features. 
```{r message=FALSE}
new_hd2 <- new_hd
new_hd2$status <- ifelse(new_hd2$status == 0, "absence", "presence")
new_hd2$status <- as.factor(new_hd2$status)
new_hd2[,-14] <- scale(new_hd2[,-14],center = TRUE, scale = TRUE)[,]
head(new_hd2)

library(ggplot2)
library(magrittr)
library(tidyr)

gather(new_hd2, x, y, age:thal) %>%
  ggplot(aes(x = y, color = status, group=status)) +
    geom_density() +
    facet_wrap( ~ x, scales = "free", ncol = 4) +
    theme(axis.title.x =element_blank(),
        axis.text.y =element_blank(),
        axis.ticks.y=element_blank())
```

#### /Categorical variables/
* Packages used: **ggplot2** (powerful plotting package), **magrittr** (%>%, decrease development time and to improve readability and maintainability of code), **tidyr** (for gather() Function)  

9 attributes are categorical variable. We transformed the attributes as factor with labels based on the data description. Density plots were shown in two sections: continuous variables and categorical variables representing how data distributed and the relationship between status and each features. The peaks of a Density Plot help display where values are concentrated over the interval. An advantage Density Plots have over Histograms is that they're better at determining the distribution shape because they're not affected by the number of bins used. The pattern of the graphs are identical to the scaled data, but the only difference is the factored data will show the labels of the data on the graphs not just numbers.
```{r message=FALSE, warning=FALSE}
new_hd3 <- new_hd
new_hd3$sex <- factor(new_hd3$sex, labels = c("female", "male"))
new_hd3$cp <- factor(new_hd3$cp, labels = c("typical", "atypical", "non-anginal", "asymptomatic"))
new_hd3$fbs <- factor(new_hd3$fbs, labels = c("False", "True"))
new_hd3$restecg <- factor(new_hd3$restecg, labels = c("normal", "abnorm", "hyper"))
new_hd3$exang <- factor(new_hd3$exang, labels = c("no", "yes"))
new_hd3$slope <- factor(new_hd3$slope, labels = c("up", "flat", "down"))
new_hd3$ca <- as.factor(new_hd3$ca)
new_hd3$thal <- factor(new_hd3$thal, labels = c("norm", "fix", "rev"))
## status: "absenece" == 0, "presence" == 1-4
new_hd3$status <- ifelse(new_hd3$status == 0, "absence", "presence")
new_hd3$status <- as.factor(new_hd3$status)
str(new_hd3)

# continuous variables
gather(new_hd3, x, y, c(age, chol, oldpeak, thalach, trestbps)) %>%
  ggplot(aes(x = y, color = status, group=status)) +
    geom_density() +
    facet_wrap( ~ x, scales = "free", ncol = 4) +
    theme(axis.title.x =element_blank(),
        axis.text.y =element_blank(),
        axis.ticks.y=element_blank())

# categorical variables
gather(new_hd3, x, y, c(sex, ca, cp, exang, fbs, restecg, slope, thal)) %>%
  ggplot(aes(x = y, color = status, group=status)) +
    geom_density() +
    facet_wrap( ~ x, scales = "free", ncol = 4) +
    theme(axis.title.x =element_blank(),
        axis.text.y =element_blank(),
        axis.ticks.y=element_blank())
```
  

### Data Exploration
#### /imbalanced response variable/
* Packages used: **ggplot2** (powerful plotting package) 

Most machine learning classification algorithm are sensitive to imbalance in the predictor classes. We have imbalanced data in the response variable. ROSE (Random Over-Sampling Examples) package will be used to provide functions to deal with binary classification problems in the presence of imbalanced classes.

```{r}
ggplot(new_hd3, aes(x = status, fill = status)) + geom_bar()
```

#### /Features/
* Packages used: **ggplot2** (powerful plotting package) 

Categorical variable and continuous variable are both in the dataset. Feature "thal" was shown as a categorical variable and feature "chol" was shown as a continuous variable.
```{r}
## thal variable
ggplot(new_hd3, aes(x = thal)) + geom_bar()
## chol variable
ggplot(new_hd3, aes(x = chol)) + geom_histogram(bins = 20)
```

#### /Principal Component Analysis/
* Packages used: **pcaGoPromoter** (principal component analysis), **ellipse** (drawing ellipses and ellipse-like confidence), **ggplot2** (powerful plotting package) 

One benefit of Principal component analysis(PCA) is that we can examine the variances associated with the principle components. Normally the first principal component has the largest possible variance, and each succedding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.
Here the 1st and 2nd component both have low variance, meaning that each dimensions are more equally important to the target data.So we can't just throw out the less important axes for dimensional reduction purpose.
```{r message=FALSE}
#setRepositories(ind = c(1:6, 8))
library(pcaGoPromoter)
library(ellipse)

# perform pca and extract scores
pcaOutput <- pca(t(new_hd2[, -14]), printDropped = FALSE, scale = TRUE, center = TRUE)
pcaOutput2 <- as.data.frame(pcaOutput$scores)
  
# define groups for plotting
pcaOutput2$groups <- new_hd2$status
  
centroids <- aggregate(cbind(PC1, PC2) ~ groups, pcaOutput2, mean)

conf.rgn  <- do.call(rbind, lapply(unique(pcaOutput2$groups), function(t)
  data.frame(groups = as.character(t),
             ellipse(cov(pcaOutput2[pcaOutput2$groups == t, 1:2]),
                   centre = as.matrix(centroids[centroids$groups == t, 2:3]),
                   level = 0.95),
             stringsAsFactors = FALSE)))
    
ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, color = groups)) + 
    geom_polygon(data = conf.rgn, aes(fill = groups), alpha = 0.2) +
    geom_point(size = 2, alpha = 0.6) + 
    scale_color_brewer(palette = "Set1") +
    labs(color = "",
         fill = "",
         x = paste0("PC1: ", round(pcaOutput$pov[1], digits = 2) * 100, "% variance"),
         y = paste0("PC2: ", round(pcaOutput$pov[2], digits = 2) * 100, "% variance")) 
```

#### /Correlation/
* Packages used: **corrplot** (correlation visulization) 

Correlations between all features are calculated and visualized with the corrplot package. 
The highest cofficient is about only 0.6, which came from the slope column, meanning the correlations among each feature are not so high.We should be very careful, When we deduct the feature.
```{r}
library(corrplot)
corr_numeric <- cor(new_hd2[,-14])
corr_numeric
corrplot(corr_numeric, order = "hclust")
```


### Data Partition
* Packages used: **caret** (for *createDataPartition()* Function), **ggplot2** (powerful plotting package), **magrittr** (*%>%*, decrease development time and to improve readability and maintainability of code), **tidyr** (for *gather()* Function)  


We splited the data into training set and testing set in the ratio of 80:20. It is good to have almost the same pattern in training and testing sets acrosss all the features.
```{r message=FALSE, warning=FALSE}
library(caret)
set.seed(100)
index <- createDataPartition(new_hd2$status,p = 0.8, list=FALSE)
train_data2 <- new_hd2[index,]
test_data2 <- new_hd2[-index,]


rbind(data.frame(group = "train", train_data2),
      data.frame(group = "test", test_data2)) %>%  
  gather(x,y,age:thal) %>%  
  ggplot(aes(x = y, color = group, fill = group)) + 
  geom_density(alpha = 0.3) + 
  facet_wrap( ~ x, scales = "free", ncol = 3) 
```

### Prediction Models - Tree-based Classification
#### /Desicion Tree/
* Packages used: **ROSE** (dealing with imbalance data), **rpart** (decision tree model), **rpart.plot** (decision tree model visulization), **caret**(For *ConfusionMatrix()* function), **ggplot2** (powerful plotting package) 

The first model is decision tree classification. We used ROSE to take care of the imbalanced class in our model. The accuracy of the model (decision_fit) is **0.8814**. We also showed the performance of the model by ConfusionMatrix and evaluating the prediction results as a histogram plot. Feature importance is listed,  *cp*, *thal*, *exang*, and *thalach* have higher significance to this model.
```{r message=FALSE, warning=FALSE}
library(ROSE)
library(rpart)
decision_fit <- rpart(status ~ ., data = train_data2, method = "class", 
                        control = rpart.control(sampling="rose"))

library(rpart.plot)
rpart.plot(decision_fit)
sum_decision_fit <- summary(decision_fit) # detailed summary of splits

# testing data validation
decision_pred <- predict(decision_fit, test_data2, type = "class")
confusionMatrix(decision_pred, test_data2$status, positive = "presence")

# plot the predict and actural value
decision_results <- data.frame(actual = test_data2$status,
                               predict = decision_pred)
decision_results$correct <- ifelse(decision_results$actual == decision_results$predict, TRUE, FALSE)
ggplot(decision_results, aes(x = predict, fill = correct)) +
  geom_bar(position = "dodge")

## Feature Importance
sum_decision_fit$variable.importance

```


#### /Random forest/
* Packages used: **ROSE** (dealing with imbalance data), **caret**(For rf model and *ConfusionMatrix()* function), **ggplot2** (powerful plotting package) 

The second model is fandom forest. We used ROSE to take care of the imbalanced class in our model. The accuracy of the model (model_rf) is **0.8814**. We also showed the performance of the model by ConfusionMatrix and evaluating the prediction results as a histogram plot and scatter plot. Feature importance is also shown as a plot. *ca*, *oldpeak*, *cp* and *thal* have higher significance to this model.
```{r message=FALSE, warning=FALSE}
set.seed(42)
model_rf <- caret::train(status ~ .,
                         data = train_data2,
                         method = "rf",
                         
                         trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE,
                                                  sampling = "rose"))

# testing data validation
model_rf_pred <- predict(model_rf, test_data2)
confusionMatrix(model_rf_pred, test_data2$status, positive = "presence")

# plot the predict and actural value
rf_results <- data.frame(actual = test_data2$status,
                      predict(model_rf, test_data2, type = "prob"))
rf_results$prediction <- ifelse(rf_results$presence > 0.5, "presence",
                             ifelse(rf_results$absence > 0.5,"absence", NA))
rf_results$correct <- ifelse(rf_results$actual == rf_results$prediction, TRUE, FALSE)

ggplot(rf_results, aes(x = prediction, fill = correct))+ 
  geom_bar(position = "dodge")

ggplot(rf_results, aes(x = prediction, y = absence, color = correct , shape = correct)) +
  geom_jitter(size = 3, alpha = 0.6)

## feature importance
rf_imp <- model_rf$finalModel$importance
rf_imp[order(rf_imp, decreasing = TRUE),]
# estimate variable importance
rf_importance <- varImp(model_rf, scale = TRUE)
plot(rf_importance)

```

#### /Extreme gradient boosting Tree/
* Packages used: **xgboost** (For the model), **caret**(For rf model and *ConfusionMatrix()* function), **ggplot2** (powerful plotting package)  


Extreme gradient boosting (XGBoost) is a faster and improved implementation of gradient boosting for supervised learning. "XGBoost uses a more regularized model formalization to control over-fitting, which gives it better performance." Tianqi Chen, developer of xgboost.
XGBoost is a tree ensemble model, which means the sum of predictions from a set of classification and regression trees (CART). In that, XGBoost is similar to Random Forests but it uses a different approach to model training. Can be used for classification and regression tasks.
In this model,we also used ROSE to deal with the imbalanced class. The accuracy of the model (model_xgb) is **0.8136**, and we evaluate the performance of the model by ConfusionMatrix, histogram plot and scatter plot. Feature importance is also shown as a plot. *ca*, *oldpeak*, *thalach* and *thal* have higher significance to this model.


```{r message=FALSE, warning=FALSE}
set.seed(142)
library(xgboost)
model_xgb <- caret::train(status ~ .,
                          data = train_data2,
                          method = "xgbTree",

                          trControl = trainControl(method = "repeatedcv",
                                                   number = 10,
                                                   repeats = 10,
                                                   savePredictions = TRUE,
                                                   #verboseIter = FALSE,
                                                   sampling = "rose"))
model_xgb

# testing data validation
xgb_pred <- predict(model_xgb, test_data2)
confusionMatrix(xgb_pred, test_data2$status, positive = "presence") 

# plot the predict and actural value
xgb_results <- data.frame(actual = test_data2$status,
                      predict(model_xgb, test_data2, type = "prob"))

xgb_results$prediction <- ifelse(xgb_results$presence> 0.5, "presence",
                             ifelse(xgb_results$absence > 0.5, "absence", NA))

xgb_results$correct <- ifelse(xgb_results$actual == xgb_results$prediction, TRUE, FALSE)

ggplot(xgb_results, aes(x = prediction, fill = correct)) +
  geom_bar(position = "dodge")

ggplot(xgb_results, aes(x = prediction, y = absence, color = correct, shape = correct)) +
  geom_jitter(size = 3, alpha = 0.6)

## Feature Importance
xgb_importance <- varImp(model_xgb, scale = TRUE)
plot(xgb_importance)
```


### Models Evaluation
* Packages used: **pROC** (For ROC curve)


Receiver Operating Characteristic curve (ROC curve) is used to provide tools to select possibly optimal models and to discard suboptimal ones, which is a direct and natural way to cost/benefit analysis of diagnostic decision making. In other words, we are not only looking at the mod
el accuracy, but also looking at the pattern generated by the true positive rate (sensitivity) and false positive rate (1-specificity) of the model. For easier comparision between models, we also calculated the area under curve (auc) for each models.The closer the auc equals to 1; the better the model is. In the result, **Random Forest** has the highest auc and accuarcy.
```{r message=FALSE}
library(pROC)
decision_roc <- roc(as.numeric(test_data2$status), as.numeric(decision_pred))
rf_roc <- roc(as.numeric(test_data2$status), as.numeric(model_rf_pred))
xgb_roc <- roc(as.numeric(test_data2$status), as.numeric(xgb_pred))

plot(decision_roc, col="red") # Red: Desicion Tree
lines(rf_roc, col="green") # Green: Random Forest
lines(xgb_roc, col= "blue") # Blue: Extreme gradient boosting Tree

data.frame(model=c("decision tree", "random forest", "Extreme gradient boosting trees"), auc = c(auc(decision_roc), auc(rf_roc), auc(xgb_roc)))
```

### Feature Selection
We could find that the importance of features are slightly different among the three models above.But they can still tell us that some of them are more important that other features.Considering high correlation coefficient of **"slope"**,basicly the most important features are *ca*, *oldpeak*, *cp* and *thal*.
We could also use an automatic way to choose features, which is Recursive Feature Elimination. RFE uses a Random Forest algorithm to test combinations of features and rate each with an accuracy score. The combination with the highest score is usually preferential.And this method leaves us three features,*cp*,*ca*,and *thal*.

```{r}
sum_decision_fit$variable.importance
plot(rf_importance)
plot(xgb_importance)

## Recursive Feature Elimination

set.seed(7)
results_rfe <- rfe(x = train_data2[,-14],
                   y = train_data2$status,
                   sizes = c(1:13),
                   rfeControl = rfeControl(functions = rfFuncs,
                                           method = "cv", number = 10)
                   )
## chosen features
predictors(results_rfe)
train_data2_rfe <- train_data2[,c(which(colnames(train_data2) %in% predictors(results_rfe)),14)]
test_data2_rfe <- test_data2[,c(which(colnames(test_data2) %in% predictors(results_rfe)),14)]
head(train_data2_rfe)

```

### Model Improvement
Then we have the data with eliminated features,and we can do grid search with *caret* and **Random Forest** again.But the result turns out no better than the original one. The ROC Curve shows that the performance of origianl data is better.
```{r}
## Grid search with caret
set.seed(42)
model_rf_tune_auto <- caret::train(status ~ .,
                         data = train_data2_rfe,
                         method = "rf",
                        # preProcess = c("scale", "center"),
                         trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE,
                                                  search = "random",
                                                  sampling = "rose"),
                         tuneLength = 15)

model_rf_tune_auto
plot(model_rf_tune_auto)

results <- data.frame(actual = test_data2_rfe$status,
                      predict(model_rf_tune_auto, test_data2_rfe, type = "prob"))

  results$prediction <- ifelse(results$presence> 0.5, "presence",
                             ifelse(results$absence > 0.5, "absence", NA))

  results$correct <- ifelse(results$actual == results$prediction, TRUE, FALSE)

ggplot(results, aes(x = prediction, fill = correct)) +
  geom_bar(position = "dodge")

ggplot(results, aes(x = prediction, y = absence, color = correct, shape = correct)) +
  geom_jitter(size = 3, alpha = 0.6)

## predicting test data
rfe_pred_2 <- predict(model_rf_tune_auto, test_data2_rfe)
confusionMatrix(rfe_pred_2,test_data2_rfe$status,positive = "presence") 

## ROC Curve and AUC comparation
ref_roc <- roc(as.numeric(test_data2_rfe$status), as.numeric(rfe_pred_2))

plot(ref_roc, col="purple")
lines(decision_roc, col="red")
lines(rf_roc, col="green")
lines(xgb_roc, col= "blue")

```


### Conclusion
There is no âONE-SIZE-FITS-ALLâ approach to ML. It is always good to know your data well before modeling like how to dealing with missing data, imbalance data, and different variable types. In so doing, we will have an idea of finding a way to do the pre-processing and feature selection in order to see if we can optimize the models. Testing different models and evaluating the effectiveness between them are also improtant when developing a meaningful model. In this dataset, although we need more tuning with the feature selection and other parameters, at this point, the random forest model has the best performance within the discussed 4 models.